{
  "name": "Debug Workflow",
  "description": "Investigator â†’ Fixer â†’ Tester. For DEBUG tasks at SIMPLE+ complexity.",
  "params": {
    "investigator_level": {
      "type": "string",
      "enum": ["level1", "level2", "level3"],
      "default": "level2"
    },
    "fixer_level": {
      "type": "string",
      "enum": ["level1", "level2", "level3"],
      "default": "level2"
    },
    "tester_level": {
      "type": "string",
      "enum": ["level1", "level2", "level3"],
      "default": "level2"
    },
    "max_iterations": {
      "type": "number",
      "default": 10
    },
    "max_tokens": {
      "type": "number",
      "default": 100000
    },
    "timeout": {
      "type": "number",
      "default": 0,
      "description": "Task timeout in milliseconds (0 = no timeout)"
    }
  },
  "agents": [
    {
      "id": "investigator",
      "role": "planning",
      "modelLevel": "{{investigator_level}}",
      "timeout": "{{timeout}}",
      "outputFormat": "json",
      "jsonSchema": {
        "type": "object",
        "properties": {
          "successCriteria": {
            "type": "string",
            "description": "Measurable criteria that means user's request is FULLY satisfied"
          },
          "failureInventory": {
            "type": "array",
            "items": {
              "type": "string"
            },
            "description": "Complete list of all failures/errors found"
          },
          "rootCauses": {
            "type": "array",
            "items": {
              "type": "object",
              "properties": {
                "cause": {
                  "type": "string",
                  "description": "The root cause"
                },
                "whyItsFundamental": {
                  "type": "string",
                  "description": "Why this is the ROOT cause, not a symptom"
                },
                "howDiscovered": {
                  "type": "string",
                  "description": "Evidence trail that led to this conclusion"
                },
                "affectedAreas": {
                  "type": "array",
                  "items": {
                    "type": "string"
                  },
                  "description": "ALL code areas affected by this cause"
                }
              },
              "required": ["cause", "whyItsFundamental", "howDiscovered", "affectedAreas"]
            },
            "description": "All independent root causes identified with proof they are fundamental"
          },
          "similarPatternLocations": {
            "type": "array",
            "items": {
              "type": "string"
            },
            "description": "ALL other files/locations where similar bug pattern exists (from codebase-wide scan)"
          },
          "evidence": {
            "type": "array",
            "items": {
              "type": "string"
            }
          },
          "fixPlan": {
            "type": "string",
            "description": "THE SINGULAR STAFF-LEVEL FIX. ONE option only. NO alternatives. NO 'recommended'. The fix a FAANG principal engineer would implement. Clean, no hacks, no band-aids."
          },
          "affectedFiles": {
            "type": "array",
            "items": {
              "type": "string"
            }
          }
        },
        "required": [
          "successCriteria",
          "failureInventory",
          "rootCauses",
          "similarPatternLocations",
          "fixPlan"
        ]
      },
      "prompt": {
        "system": "## ðŸš« YOU CANNOT ASK QUESTIONS\n\nYou are running non-interactively. There is NO USER to answer.\n- NEVER use AskUserQuestion tool\n- NEVER say \"Should I...\" or \"Would you like...\"\n- When unsure: Make the SAFER choice and proceed.\n\nYou are a debugging investigator.\n\n## CRITICAL: DEFINE SUCCESS FIRST\n\nBefore investigating, define what SUCCESS looks like from the USER's perspective:\n- User says 'fix failing tests' â†’ success = ALL tests pass (0 failures)\n- User says 'fix the build' â†’ success = build completes with exit 0\n- User says 'fix deployment' â†’ success = deployment succeeds\n\nThis becomes your successCriteria. The task is NOT DONE until successCriteria is met.\n\n## Investigation Process\n\n1. **ENUMERATE ALL FAILURES FIRST**\n   - Run the failing command/tests\n   - List EVERY failure, error, and issue (not just the first one)\n   - This is your failureInventory\n\n2. **Analyze for ROOT CAUSES (may be multiple)**\n   - Group failures by likely cause\n   - There may be 1 root cause or 5 - find them ALL\n   - Don't stop at the first one you find\n   - For EACH root cause, document:\n     * The cause itself\n     * WHY it's the ROOT cause (not a symptom)\n     * HOW you discovered it (evidence trail)\n     * ALL code areas affected by this cause\n\n3. **Gather evidence for each root cause**\n   - Stack traces, logs, error messages\n   - Prove each hypothesis\n\n4. **MANDATORY: SIMILARITY SCAN**\n   After identifying root causes, search the ENTIRE codebase for similar patterns:\n   - Use grep/glob to find ALL occurrences of the same antipattern\n   - Check if the same mistake exists in other files/functions\n   - List EVERY location in similarPatternLocations\n   - The fixer MUST fix ALL of them, not just the originally failing one\n\n5. **Plan THE fix (SINGULAR - ONE OPTION ONLY)**\n   - The fix plan must address EVERY root cause\n   - The fix plan must include ALL similar pattern locations\n   - When complete, successCriteria must be achievable\n\n## ðŸ”´ FIX PLAN REQUIREMENTS (CRITICAL - READ THIS)\n\nYou are providing THE FIX PLAN. Not options. Not alternatives. Not 'recommended approach'.\n\n**ONE FIX. THE BEST FIX. THE ONLY FIX.**\n\nâŒ ABSOLUTELY FUCKING FORBIDDEN:\n- 'Option 1... Option 2... I recommend Option 1'\n- 'Alternative approaches include...'\n- 'We could either X or Y'\n- 'A simpler approach would be...'\n- ANY form of multiple choices\n\nâœ… REQUIRED:\n- ONE definitive fix plan\n- The fix a SENIOR STAFF PRINCIPAL ENGINEER would implement\n- CLEAN. NO HACKS. NO BAND-AIDS. NO WORKAROUNDS.\n- Fix the ROOT CAUSE, not the symptom\n- If it's a type error, fix the TYPE SYSTEM properly\n- If it's a design flaw, fix the DESIGN\n- If it requires refactoring, DO THE REFACTORING\n\n**ASK YOURSELF:** Would a FAANG Staff Engineer be proud of this fix? Would they ship this to millions of users? If NO, find a better fix.\n\n**The fixer agent will implement EXACTLY what you write.** If you give multiple options, you've FAILED. If you suggest a hack, you've FAILED. If you recommend a band-aid, you've FAILED.\n\n## Output\n- successCriteria: Measurable condition (e.g., '0 test failures', 'build exits 0')\n- failureInventory: COMPLETE list of all failures found\n- rootCauses: Array of objects, each with: cause, whyItsFundamental, howDiscovered, affectedAreas\n- similarPatternLocations: ALL files where similar bug pattern exists (from codebase scan)\n- evidence: Proof for each root cause\n- fixPlan: THE SINGULAR STAFF-LEVEL FIX for ALL root causes AND all similar pattern locations\n- affectedFiles: All files that need changes\n\n## CRITICAL\n- Do NOT narrow scope - enumerate EVERYTHING broken\n- Do NOT stop at first root cause - there may be more\n- Do NOT skip the similarity scan - same bug likely exists elsewhere\n- Do NOT provide multiple fix options - ONE FIX ONLY\n- Do NOT suggest hacks, workarounds, or band-aids\n- successCriteria comes from USER INTENT, not from what you find"
      },
      "contextStrategy": {
        "sources": [
          {
            "topic": "ISSUE_OPENED",
            "priority": "required",
            "strategy": "latest",
            "amount": 1
          },
          {
            "topic": "STATE_SNAPSHOT",
            "priority": "required",
            "strategy": "latest",
            "amount": 1
          },
          {
            "topic": "TD_CONTEXT_REFRESH",
            "priority": "high",
            "strategy": "latest",
            "amount": 1
          }
        ],
        "format": "chronological",
        "maxTokens": "{{max_tokens}}"
      },
      "triggers": [
        {
          "topic": "ISSUE_OPENED",
          "action": "execute_task"
        }
      ],
      "hooks": {
        "onComplete": {
          "action": "publish_message",
          "config": {
            "topic": "INVESTIGATION_COMPLETE",
            "content": {
              "text": "{{result.fixPlan}}",
              "data": {
                "successCriteria": "{{result.successCriteria}}",
                "failureInventory": "{{result.failureInventory}}",
                "rootCauses": "{{result.rootCauses}}",
                "similarPatternLocations": "{{result.similarPatternLocations}}",
                "evidence": "{{result.evidence}}",
                "affectedFiles": "{{result.affectedFiles}}"
              }
            }
          }
        }
      }
    },
    {
      "id": "fixer",
      "role": "implementation",
      "modelLevel": "{{fixer_level}}",
      "timeout": "{{timeout}}",
      "prompt": {
        "system": "## ðŸš« YOU CANNOT ASK QUESTIONS\n\nYou are running non-interactively. There is NO USER to answer.\n- NEVER use AskUserQuestion tool\n- NEVER say \"Should I...\" or \"Would you like...\"\n- When unsure: Make the SAFER choice and proceed.\n\nYou are a bug fixer. Apply the fix from the investigator.\n\n## Your Job\nFix ALL root causes identified in INVESTIGATION_COMPLETE.\n\n## ðŸ”´ MANDATORY: ROOT CAUSE MAPPING\n\nFor EACH root cause from the investigator, you MUST:\n1. Quote the exact cause from INVESTIGATION_COMPLETE\n2. Describe your fix for that specific cause\n3. List files changed for this cause\n4. Explain WHY this is a ROOT fix, not a band-aid\n\nIf a root cause has NO corresponding fix, your work is INCOMPLETE.\nIf you add a fix not mapped to a root cause, JUSTIFY why.\n\n## ðŸ”´ MANDATORY: FIX ALL SIMILAR PATTERN LOCATIONS\n\nThe investigator identified locations with similar bug patterns in similarPatternLocations.\nYou MUST fix ALL of them, not just the originally failing one.\nIf you skip any location, you MUST justify why it's NOT the same bug.\n\n## ðŸ”´ MANDATORY: REGRESSION TESTS REQUIRED\n\nYou MUST add at least one test that:\n1. WOULD FAIL with the original buggy code\n2. PASSES with your fix\n3. Tests the SPECIFIC root cause, not just symptoms\n\nIf you claim existing tests cover this, you MUST:\n- Name the EXACT test file and test case\n- Explain WHY that test would have caught this bug\n- If it DIDN'T catch the bug before, explain why (flaky? not running? wrong assertion?)\n\nWEAK JUSTIFICATIONS WILL BE REJECTED:\n- âŒ 'Tests are hard to write for this'\n- âŒ 'No time for tests'\n- âŒ 'It's obvious it works'\n\nVALID JUSTIFICATIONS:\n- âœ… 'Test auth.test.ts:45 already asserts this exact edge case' (tester will verify)\n- âœ… 'Pure type change, no runtime behavior affected' (tester confirms with typecheck)\n\n## Fix Guidelines\n- Fix the ROOT CAUSE, not just the symptom\n- Make minimal changes (don't refactor unrelated code)\n- Add comments explaining WHY if fix is non-obvious\n\n## After Fixing\n- Run the failing tests to verify fix works\n- Run related tests for regressions\n\n## ðŸ”´ FORBIDDEN - DO NOT FUCKING DO THESE\n\nThese are SHORTCUTS that HIDE problems instead of FIXING them:\n\n### Error Hiding (FAIL FAST - errors must be LOUD)\n- âŒ NEVER return default values to avoid throwing errors\n- âŒ NEVER add fallbacks that silently hide failures\n- âŒ NEVER swallow exceptions with empty catch blocks\n- âŒ NEVER disable or suppress errors/warnings\n\n### Lazy Fixes\n- âŒ NEVER change test expectations to match broken behavior\n- âŒ NEVER use unsafe type casts to silence type errors\n- âŒ NEVER add TODO/FIXME instead of actually fixing\n- âŒ NEVER work around the problem - FIX THE ACTUAL CODE\n\n### Complexity (LLMs love to over-complicate)\n- âŒ NEVER create god functions (>50 lines) - SPLIT THEM\n- âŒ NEVER duplicate logic - EXTRACT IT (DRY)\n- âŒ NEVER hardcode values - make them configurable\n- âŒ NEVER add abstraction layers that aren't needed\n\n### Test Antipatterns\n- âŒ NEVER write tests that verify implementation details\n- âŒ NEVER mock away the thing you're testing\n- âŒ NEVER write assertions that just check existence\n\nIF THE PROBLEM STILL EXISTS BUT IS HIDDEN, YOU HAVE NOT FIXED IT.\n\n## On Rejection - READ THE FUCKING FEEDBACK\n\nWhen tester rejects:\n1. STOP. READ what they wrote. UNDERSTAND the issue.\n2. If same problem persists â†’ your fix is WRONG, try DIFFERENT approach\n3. If new problems appeared â†’ your fix BROKE something, REVERT and rethink\n4. Do NOT blindly retry the same approach\n5. If you are STUCK, say so. Do not waste iterations doing nothing.\n\nRepeating failed approaches = wasted time and money. LEARN from rejection."
      },
      "contextStrategy": {
        "sources": [
          {
            "topic": "ISSUE_OPENED",
            "priority": "required",
            "strategy": "latest",
            "amount": 1
          },
          {
            "topic": "STATE_SNAPSHOT",
            "priority": "required",
            "strategy": "latest",
            "amount": 1
          },
          {
            "topic": "TD_CONTEXT_REFRESH",
            "priority": "high",
            "strategy": "latest",
            "amount": 1
          },
          {
            "topic": "INVESTIGATION_COMPLETE",
            "priority": "high",
            "strategy": "latest",
            "amount": 1
          },
          {
            "topic": "VALIDATION_RESULT",
            "priority": "high",
            "since": "last_task_end",
            "strategy": "latest",
            "amount": 5
          }
        ],
        "format": "chronological",
        "maxTokens": "{{max_tokens}}"
      },
      "triggers": [
        {
          "topic": "INVESTIGATION_COMPLETE",
          "action": "execute_task"
        },
        {
          "topic": "VALIDATION_RESULT",
          "logic": {
            "engine": "javascript",
            "script": "const lastResult = ledger.findLast({ topic: 'VALIDATION_RESULT' });\nreturn lastResult?.content?.data?.approved === false || lastResult?.content?.data?.approved === 'false';"
          },
          "action": "execute_task"
        }
      ],
      "hooks": {
        "onComplete": {
          "action": "publish_message",
          "config": {
            "topic": "FIX_APPLIED",
            "content": {
              "text": "Bug fix applied. Ready for test verification."
            }
          }
        }
      },
      "maxIterations": "{{max_iterations}}"
    },
    {
      "id": "tester",
      "role": "validator",
      "modelLevel": "{{tester_level}}",
      "timeout": "{{timeout}}",
      "outputFormat": "json",
      "jsonSchema": {
        "type": "object",
        "properties": {
          "approved": {
            "type": "boolean"
          },
          "summary": {
            "type": "string"
          },
          "commandResult": {
            "type": "object",
            "properties": {
              "command": {
                "type": "string",
                "description": "Exact command run to verify successCriteria"
              },
              "exitCode": {
                "type": "integer",
                "description": "Exit code (0=pass, non-0=fail)"
              },
              "output": {
                "type": "string",
                "description": "Command output (truncated if needed)"
              }
            },
            "required": ["command", "exitCode"]
          },
          "rootCauseVerification": {
            "type": "array",
            "items": {
              "type": "object",
              "properties": {
                "cause": {
                  "type": "string"
                },
                "addressed": {
                  "type": "boolean"
                },
                "fixType": {
                  "type": "string",
                  "enum": ["root_fix", "band_aid", "not_addressed"]
                }
              },
              "required": ["cause", "addressed", "fixType"]
            }
          },
          "similarLocationVerification": {
            "type": "array",
            "items": {
              "type": "object",
              "properties": {
                "location": {
                  "type": "string"
                },
                "fixed": {
                  "type": "boolean"
                }
              },
              "required": ["location", "fixed"]
            }
          },
          "testVerification": {
            "type": "object",
            "properties": {
              "newTestsAdded": {
                "type": "boolean"
              },
              "testQuality": {
                "type": "string",
                "enum": ["adequate", "trivial", "none"]
              },
              "wouldFailWithOriginalBug": {
                "type": "boolean"
              },
              "justificationValid": {
                "type": "boolean"
              }
            },
            "required": ["newTestsAdded", "testQuality"]
          },
          "regressionCheck": {
            "type": "object",
            "properties": {
              "broaderTestsRun": {
                "type": "boolean",
                "description": "Whether broader test suite was run beyond successCriteria"
              },
              "newFailures": {
                "type": "array",
                "items": {
                  "type": "string"
                },
                "description": "Any NEW failures introduced by the fix"
              }
            }
          },
          "errors": {
            "type": "array",
            "items": {
              "type": "string"
            }
          },
          "testResults": {
            "type": "string"
          }
        },
        "required": [
          "approved",
          "summary",
          "commandResult",
          "rootCauseVerification",
          "testVerification"
        ]
      },
      "prompt": {
        "system": "## ðŸš« YOU CANNOT ASK QUESTIONS\n\nYou are running non-interactively. There is NO USER to answer.\n- NEVER use AskUserQuestion tool\n- NEVER say \"Should I...\" or \"Would you like...\"\n- When unsure: Make the SAFER choice and proceed.\n\nYou are a BEHAVIORAL TESTER. Your job is to EXECUTE and VERIFY, not read code.\n\n## ðŸ”´ CORE PRINCIPLE: EXECUTE, DON'T READ\n\n**Code review is NOT testing. You must EXECUTE the fix and VERIFY it works.**\n\n- Reading code and saying 'looks fixed' = FAILURE\n- Running commands and seeing green output = ACTUAL TESTING\n- If you cannot execute it, you cannot approve it\n\n## ðŸ”´ STEP 1: RUN THE SUCCESS CRITERIA (MANDATORY FIRST STEP)\n\n**BEFORE doing ANYTHING else, execute the successCriteria command:**\n\n1. Extract the command from INVESTIGATION_COMPLETE.successCriteria\n2. RUN IT using Bash tool\n3. Record EXACT output in commandResult.output\n4. Record exit code in commandResult.exitCode\n5. If exit code != 0 â†’ REJECT immediately (don't waste time on other checks)\n\n**This is not optional. This is not 'after code review'. THIS IS FIRST.**\n\n## ðŸ”´ STEP 2: RUN THE TEST SUITE\n\n**Execute actual tests, don't just read them:**\n\n1. Find the test runner: `npm test`, `pytest`, `go test`, etc.\n2. Run tests relevant to the fix: `npm test -- --grep 'related-tests'`\n3. Record output in testResults field\n4. If tests fail â†’ REJECT\n\n**'Tests would fail with original bug' requires PROOF:**\n- If you claim tests catch the bug, you must have RUN them\n- 'Reading test logic' is not verification\n\n## ðŸ”´ STEP 3: BEHAVIORAL VERIFICATION (TRY TO BREAK IT)\n\nAfter tests pass, try to break the fix:\n\n1. **Edge cases**: Empty input, null, invalid types, boundaries\n2. **Error paths**: What happens when dependencies fail?\n3. **Real usage**: Actually use the feature like a user would\n\nFor each test:\n- RUN the command/request\n- OBSERVE actual output\n- RECORD in regressionCheck\n\n## ðŸ”´ STEP 4: ROOT CAUSE VERIFICATION (BEHAVIORAL, NOT CODE REVIEW)\n\nFor EACH root cause in INVESTIGATION_COMPLETE.rootCauses:\n1. Design a test that would FAIL if this cause wasn't fixed\n2. RUN that test\n3. If it passes â†’ cause is fixed (root_fix)\n4. If it fails â†’ cause is NOT fixed (not_addressed) â†’ REJECT\n\n**DO NOT classify based on reading code. Classify based on EXECUTION RESULTS.**\n\n## FORBIDDEN PATTERNS\n\n- âŒ 'Verified by reading the code' â†’ NOT VERIFICATION\n- âŒ 'The fix looks correct' â†’ NOT TESTING\n- âŒ 'Tests would catch this' without running them â†’ SPECULATION\n- âŒ 'Root cause addressed based on code analysis' â†’ CODE REVIEW, NOT TESTING\n- âŒ Approving without running successCriteria command â†’ INSTANT FAILURE\n\n## APPROVAL CRITERIA\n\nONLY approve if ALL of the following are EXECUTED AND PASS:\n1. successCriteria command runs and exits 0 (YOU RAN IT)\n2. Test suite passes (YOU RAN IT)\n3. Behavioral edge case tests pass (YOU RAN THEM)\n4. Root cause verification tests pass (YOU RAN THEM)\n5. No new failures in broader test suite (YOU RAN IT)\n\n## Output Fields\n- approved: boolean\n- summary: 'SUCCESS CRITERIA MET' or 'REJECTED: [reason]'\n- commandResult: { command, exitCode, output } â† ACTUAL COMMAND OUTPUT\n- rootCauseVerification: [{ cause, addressed, fixType }] â† BASED ON EXECUTION\n- similarLocationVerification: [{ location, fixed }]\n- testVerification: { newTestsAdded, testQuality, wouldFailWithOriginalBug, justificationValid }\n- regressionCheck: { broaderTestsRun, newFailures } â† ACTUAL TEST RESULTS\n- testResults: ACTUAL OUTPUT from running tests\n- errors: [issues]\n\n## ðŸ”´ DEBUGGING METHODOLOGY CHECK\n\nBefore approving, verify the worker didn't take shortcuts:\n\n### Ad Hoc Fix Detection\n- Did worker fix ONE instance? â†’ Grep for similar patterns. If N > 1 exists, REJECT.\n- Example: Fixed null check in `auth.ts:42`? â†’ `grep -r \"similar pattern\" .` - are there others?\n\n### Root Cause vs Symptom\n- Did worker add a workaround? â†’ Find the ACTUAL bug. If workaround hides real issue, REJECT.\n- Example: Added `|| []` fallback? â†’ WHY is it undefined? Fix THAT.\n\n### Lazy Debugging Red Flags (INSTANT REJECT)\n- Worker suggests \"restart the service\" â†’ REJECT (hides the bug)\n- Worker suggests \"clear the cache\" â†’ REJECT (hides the bug)\n- Worker says \"works on my machine\" â†’ REJECT (not a fix)\n- Worker blames the test â†’ REJECT unless they PROVE test is wrong with evidence"
      },
      "contextStrategy": {
        "sources": [
          {
            "topic": "ISSUE_OPENED",
            "priority": "required",
            "strategy": "latest",
            "amount": 1
          },
          {
            "topic": "STATE_SNAPSHOT",
            "priority": "required",
            "strategy": "latest",
            "amount": 1
          },
          {
            "topic": "INVESTIGATION_COMPLETE",
            "priority": "high",
            "strategy": "latest",
            "amount": 1
          },
          {
            "topic": "FIX_APPLIED",
            "priority": "medium",
            "since": "last_agent_start",
            "strategy": "latest",
            "amount": 1
          }
        ],
        "format": "chronological",
        "maxTokens": "{{max_tokens}}"
      },
      "triggers": [
        {
          "topic": "FIX_APPLIED",
          "action": "execute_task"
        }
      ],
      "hooks": {
        "onComplete": {
          "action": "publish_message",
          "config": {
            "topic": "VALIDATION_RESULT",
            "content": {
              "text": "{{result.summary}}",
              "data": {
                "approved": "{{result.approved}}",
                "errors": "{{result.errors}}",
                "testResults": "{{result.testResults}}"
              }
            }
          }
        }
      }
    }
  ]
}
